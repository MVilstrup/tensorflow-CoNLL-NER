{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import all the necessary packages\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "import random\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "from input import get_train_data,get_test_data,get_final_data\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data into memory\n",
      "test_a data loaded\n",
      "test_b data loaded\n",
      "train data loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load data into memory \n",
    "\"\"\"\n",
    "print \"loading data into memory\"\n",
    "pool = Pool(processes=3)\n",
    "train_result = pool.apply_async(get_train_data)\n",
    "test_a_result = pool.apply_async(get_test_data)\n",
    "test_b_result = pool.apply_async(get_final_data)\n",
    "\n",
    "test_inp, test_out = test_a_result.get()\n",
    "print \"test_a data loaded\"\n",
    "\n",
    "final_inp, final_out = test_b_result.get()\n",
    "print \"test_b data loaded\"\n",
    "\n",
    "train_inp, train_out = train_result.get()\n",
    "print \"train data loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
      "[ 0.00753893  0.4429522   0.6082194   0.47846727  0.63072812  0.65132619\n",
      "  0.0244708   0.82934638  0.63344647  0.45958061  0.21532162  0.48766874\n",
      "  0.99585648  0.89168342  0.42134235  0.54405465  0.94602753  0.20298426\n",
      "  0.83931349  0.13234238  0.76526675  0.36424376  0.78451387  0.24423664\n",
      "  0.40405354  0.94618541  0.64868175  0.35889811  0.83237328  0.75670462\n",
      "  0.58740005  0.63011902  0.33657386  0.55998569  0.23590601  0.15847046\n",
      "  0.07705077  0.52542988  0.71014924  0.49015792  0.37892592  0.27533119\n",
      "  0.24534198  0.22301109  0.28631221  0.19378396  0.94386822  0.82044902\n",
      "  0.10530504  0.17121434  0.30754337  0.86034054  0.61336142  0.84412731\n",
      "  0.61880204  0.11399117  0.37042367  0.68083447  0.69293952  0.56450751\n",
      "  0.73611949  0.41860562  0.62613314  0.83870405  0.95957705  0.23850788\n",
      "  0.09996017  0.58575166  0.01141853  0.35075413  0.09235751  0.01974887\n",
      "  0.44969944  0.81352404  0.03171994  0.79232107  0.88424554  0.79186866\n",
      "  0.48763762  0.48100099  0.57877265  0.94521146  0.09214985  0.05137587\n",
      "  0.32381178  0.74383094  0.30953148  0.39004912  0.95107203  0.30597917\n",
      "  0.61672797  0.29231747  0.09080059  0.51062077  0.08144613  0.88239476\n",
      "  0.33627254  0.64326939  0.7392513   0.75347811  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyse the data to m\n",
    "\"\"\"\n",
    "print([len(f) for f in test_inp[:10]])\n",
    "print(test_inp[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Methods\n",
    "\"\"\"\n",
    "\n",
    "def weight_and_bias(in_size,out_size):\n",
    "        weight = tf.truncated_normal([in_size,out_size], stddev=0.01, name=\"weight\")\n",
    "        bias = tf.constant(0.1, shape=[out_size], name=\"bias\")\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "    \n",
    "def length(data):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(data), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "    \n",
    "\n",
    "def error():\n",
    "    mistakes = tf.not_equal(tf.argmax(target, 2), tf.argmax(prediction, 2))\n",
    "\n",
    "    mistakes = tf.cast(mistakes, tf.float32)\n",
    "    mask = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2))\n",
    "    mistakes *= mask\n",
    "    # Average over actual sequence lengths.\n",
    "    mistakes = tf.reduce_sum(mistakes, reduction_indices=1)\n",
    "    mistakes /= tf.cast(length(data), tf.float32)\n",
    "    return tf.reduce_mean(mistakes)\n",
    "\n",
    "\"\"\"\n",
    "F1-prediction function\n",
    "\"\"\"\n",
    "def f1(prediction, target, length):\n",
    "    tp=np.array([0]*(NUM_CLASSES+1))\n",
    "    fp=np.array([0]*(NUM_CLASSES+1))\n",
    "    fn=np.array([0]*(NUM_CLASSES+1))\n",
    "\n",
    "    target = np.argmax(target, 2)\n",
    "    prediction = np.argmax(prediction, 2)\n",
    "\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        for j in range(length[i]):\n",
    "            if target[i][j] == prediction[i][j]:\n",
    "                tp[target[i][j]] += 1\n",
    "            else:\n",
    "                fp[target[i][j]] += 1\n",
    "                fn[prediction[i][j]] += 1\n",
    "\n",
    "    NON_NAMED_ENTITY = 11\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if i != NON_NAMED_ENTITY:\n",
    "            tp[NUM_CLASSES] += tp[i]\n",
    "            fp[NUM_CLASSES] += fp[i]\n",
    "            fn[NUM_CLASSES] += fn[i]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    for i in range(NUM_CLASSES+1):\n",
    "        precision.append(tp[i]*1.0/(tp[i]+fp[i]))\n",
    "        recall.append(tp[i]*1.0/(tp[i]+ fn[i]))\n",
    "        fscore.append(2.0*precision[i]*recall[i]/(precision[i]+recall[i]))\n",
    "\n",
    "    print \"precision = {}\".format([\"{:10.4f}%\".format(f) for f in precision])\n",
    "    print \"recall = {}\".format([\"{:10.4f}%\".format(f) for f in recall])\n",
    "    print \"f1score = {}\".format([\"{:10.4f}%\".format(f) for f in fscore])\n",
    "                            \n",
    "    return fscore[NUM_CLASSES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining variables\n",
      "data_pl, (?, 30, 113)\n",
      "target_pl, (?, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define all the model varaibles\n",
    "\"\"\"\n",
    "WORD_DIM = 113\n",
    "MAX_SEQ_LEN = 30\n",
    "NUM_CLASSES = 9\n",
    "NUM_HIDDEN = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "# resetting the graph before creating a new one\n",
    "reset_default_graph()\n",
    "\n",
    "print(\"defining variables\")\n",
    "\"\"\"\n",
    "Initialize the placeholders needed to input the data\n",
    "\"\"\"        \n",
    "\n",
    "data = tf.placeholder(tf.float32,[None, MAX_SEQ_LEN, WORD_DIM], name=\"data_placeholder\")\n",
    "target = tf.placeholder(tf.float32, [None, MAX_SEQ_LEN, NUM_CLASSES], name=\"target_placeholder\")\n",
    "\n",
    "print \"data_pl,\", data.get_shape()\n",
    "print \"target_pl,\", target.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the tensorflow model used to train the NER reacogniser\n",
    "\"\"\"\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell\n",
    "# Try: LSTMBlock cell or GruBlock cell\n",
    "fw_cell = rnn_cell.LSTMCell(NUM_HIDDEN, state_is_tuple=True)\n",
    "bw_cell = rnn_cell.LSTMCell(NUM_HIDDEN, state_is_tuple=True)\n",
    "\n",
    "if NUM_LAYERS > 1:\n",
    "    fw_cell = rnn_cell.MultiRNNCell([fw_cell] * NUM_LAYERS, state_is_tuple=True)\n",
    "    fw_cell = rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=DROPOUT)\n",
    "    bw_cell = rnn_cell.MultiRNNCell([bw_cell] * NUM_LAYERS, state_is_tuple=True)\n",
    "    bw_cell = rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=DROPOUT)\n",
    "else:\n",
    "    fw_cell = rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=DROPOUT)\n",
    "    bw_cell = rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=DROPOUT)\n",
    "\n",
    "# Try: Dynamic Bidirectional RNN\n",
    "output, _, _ = tf.nn.bidirectional_rnn(fw_cell, \n",
    "                                       bw_cell, \n",
    "                                       tf.unpack(tf.transpose(data, perm=[1, 0, 2])), \n",
    "                                       dtype=tf.float32, \n",
    "                                       sequence_length=length(data))\n",
    "\n",
    "max_length = int(target.get_shape()[1])\n",
    "num_classes = int(target.get_shape()[2])\n",
    "weight, bias = weight_and_bias(2*NUM_HIDDEN, num_classes)\n",
    "output = tf.reshape(tf.transpose(tf.pack(output), perm=[1, 0, 2]), [-1, 2 * NUM_HIDDEN], name=\"generate_output\")\n",
    "prediction = tf.nn.softmax(tf.matmul(output, weight) + bias, name=\"generate_prediction\")\n",
    "prediction = tf.reshape(prediction, [-1, max_length, num_classes], name=\"reshape_prediction\")\n",
    "\n",
    "# add TensorBoard summaries for all variables\n",
    "tf.contrib.layers.summarize_variables()\n",
    "\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "# initialize the Session\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "\n",
    "# test the forward pass\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# setup and write summaries\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "summaries_path = \"tensorboard/%s/logs\" % (timestamp)\n",
    "summaries = tf.merge_all_summaries()\n",
    "summarywriter = tf.train.SummaryWriter(summaries_path, sess.graph)\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how the Network should calculate the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary_3:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define the Cost function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def loss_and_acc(prediction):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(target * tf.log(prediction+1e-10), reduction_indices=2)\n",
    "    \n",
    "    # Check if the maximum value on the secondary axis is positive or negative\n",
    "    mask = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2)) \n",
    "    cross_entropy *= mask # Ensure the cross_entropy is positive (by multiplying either with -1 or 1)\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1) # Summarize the values on the primary axis\n",
    "    cross_entropy /= tf.cast(length(data), tf.float32) # Convert all dimensions of the vector to 32float.\n",
    "    \n",
    "    # averaging over samples\n",
    "    loss = tf.reduce_mean(cross_entropy) # Reduce the vector to the mean value on all dimensions\n",
    "    \n",
    "    # if you want regularization\n",
    "    #reg_scale = 0.0001\n",
    "    #regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    #params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    #reg_term = sum([regularize(param) for param in params])\n",
    "    #loss += reg_term\n",
    "    \n",
    "    # calculate accuracy\n",
    "    argmax_pred = tf.to_int32(tf.argmax(prediction, dimension=1))\n",
    "    argmax_target = tf.to_int32(tf.argmax(target, dimension=1))\n",
    "    correct = tf.to_float(tf.equal(argmax_pred, argmax_target))\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    return loss, accuracy, argmax_pred\n",
    "\n",
    "# loss, accuracy and prediction\n",
    "loss, accuracy, prediction = loss_and_acc(prediction)\n",
    "\n",
    "loss_valid = loss\n",
    "accuracy_valid = accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make tensorboard summeries\n",
    "tf.scalar_summary('train/loss', loss)\n",
    "tf.scalar_summary('train/accuracy', accuracy)\n",
    "tf.scalar_summary('validation/loss', loss_valid)\n",
    "tf.scalar_summary('validation/accuracy', accuracy_valid)\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how the network should optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "clip_norm = 1\n",
    "# defining our optimizer and\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# applying the gradients\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "clipped_gradients, global_norm = (tf.clip_by_global_norm(gradients, clip_norm))\n",
    "clipped_grads_and_vars = zip(clipped_gradients, variables)\n",
    "\n",
    "# make training operation for applying the gradients\n",
    "train_operation = optimizer.apply_gradients(clipped_grads_and_vars)\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value beta2_power_1\n\t [[Node: beta2_power_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@BiRNN_FW/MultiRNNCell/Cell0/LSTMCell/W_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](beta2_power_1)]]\nCaused by op u'beta2_power_1/read', defined at:\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-43-4ee17666bf47>\", line 12, in <module>\n    train_operation = optimizer.apply_gradients(clipped_grads_and_vars)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 300, in apply_gradients\n    self._create_slots(var_list)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 115, in _create_slots\n    trainable=False)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 323, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1098, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-25d4f201d51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfetch_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mreturn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Give the session what it should return as well as inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# Store the loss at every run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# Store the accuracy at every run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value beta2_power_1\n\t [[Node: beta2_power_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@BiRNN_FW/MultiRNNCell/Cell0/LSTMCell/W_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](beta2_power_1)]]\nCaused by op u'beta2_power_1/read', defined at:\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-43-4ee17666bf47>\", line 12, in <module>\n    train_operation = optimizer.apply_gradients(clipped_grads_and_vars)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 300, in apply_gradients\n    self._create_slots(var_list)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 115, in _create_slots\n    trainable=False)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 323, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1098, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define the methods needed to train the model\n",
    "\"\"\"\n",
    "\n",
    "NUM_EPOCH = 100 # How many times we iterate through the dataset\n",
    "BATCH_SIZE = 256 # How many sentences we analyse at a time\n",
    "BATCH_AMOUNT = (len(train_inp) + BATCH_SIZE - 1) / BATCH_SIZE\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "print(\"starting training\")\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    ptr = 0 # Used to figure what data have already been processed\n",
    "    for batch_number in range(BATCH_AMOUNT):\n",
    "        batch_inp, batch_out = train_inp[ptr:ptr+BATCH_SIZE], train_out[ptr:ptr+BATCH_SIZE]\n",
    "        ptr += BATCH_SIZE\n",
    "        \n",
    "        # Define the input that should be given to the neural network at each run\n",
    "        feed_values = {\n",
    "            data: np.array(batch_inp),\n",
    "            target: np.array(batch_out)\n",
    "        }\n",
    "        # Define the values that should be extracted from the neural network at each run\n",
    "        fetch_values = [train_operation, loss, accuracy, summaries]\n",
    "        \n",
    "        return_values = sess.run(fetch_values, feed_values) # Give the session what it should return as well as inputs\n",
    "        train_loss.append(return_values[1])                 # Store the loss at every run\n",
    "        train_acc.append(return_values[2])                  # Store the accuracy at every run\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    pred = sess.run(model.prediction, {data: test_inp, target: test_out, dropout: 1})\n",
    "    pred,length = sess.run(model.getpredf1, {data: test_inp, target: test_out, dropout: 1})\n",
    "    print \"Epoch:\" + str(epoch), \"TestA score,\"\n",
    "    m = f1(pred,test_out,length)\n",
    "    if m > maximum:\n",
    "        maximum = m\n",
    "        save_path = saver.save(sess, \"model/model_max.ckpt\")\n",
    "        print(\"Max Model saved in file: %s\" % save_path)\n",
    "        pred = sess.run(model.prediction, {data: final_inp, target: final_out, dropout: 1})\n",
    "        pred,length = sess.run(model.getpredf1, {data: final_inp, target: final_out, dropout: 1})\n",
    "        print \"TestB score,\"\n",
    "        f1(pred, final_out, length)\n",
    "        print\"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
