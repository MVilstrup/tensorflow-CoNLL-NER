{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vilstrup/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "Import all the necessary packages\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "import random\n",
    "import argparse\n",
    "from multiprocessing import Pool, Process, cpu_count\n",
    "import random\n",
    "import pickle as pkl\n",
    "import os\n",
    "import codecs\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from IPython import display\n",
    "import pandas as pd \n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs \n",
    "from tensorflow.python.framework import ops\n",
    "import tflearn\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell, BasicLSTMCell, GRUCell\n",
    "from tensorflow.python.ops import rnn\n",
    "import numpy as np\n",
    "from numpy import float32\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.python.framework.ops import reset_default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_training_file(training_file):\n",
    "    \"\"\"\n",
    "    Analyse the amount of features in the training dataset. The data should\n",
    "    look like the following:\n",
    "\n",
    "    Mikkel feat1 feat2 ... B-PERS\n",
    "    Vilstrup feat1 feat2 ... I-PERS\n",
    "    is feat1 feat2 ... O\n",
    "    the feat1 feat2 ... O\n",
    "    author feat1 feat2 ... O\n",
    "    ...\n",
    "\n",
    "    where feat1 feat2 ... are features related to the word on the left\n",
    "    It is assumed that the features never change position, and that each feature\n",
    "    is seperated by a space (\" \").\n",
    "    \"\"\"\n",
    "    lines = codecs.open(training_file, encoding=\"utf-8\").readlines()\n",
    "    lines = [l for l in lines if \"-DOCSTART-\" not in l]\n",
    "\n",
    "    num_features = 0\n",
    "    for line in lines:\n",
    "        num_features = max(num_features, len(line.split()) - 1) # find the maximum amount of features\n",
    "\n",
    "    features = [{} for i in range(num_features)] # create a dictionary for each feature\n",
    "\n",
    "    max_length = 0\n",
    "    current_length = 0\n",
    "    # Iterate through all the lines to get the categories of each feature\n",
    "    for line in lines:\n",
    "        if line in ['\\n', '\\r\\n']:\n",
    "            # this is the end of a sentence.\n",
    "            max_length = min(MAX_SENTENCE_LENGTH, max(max_length, current_length))\n",
    "            current_length = 0\n",
    "            continue\n",
    "        else:\n",
    "            current_length +=1\n",
    "            words = line.split()[1:] # discard the word on the left\n",
    "\n",
    "            for index, word in enumerate(words):\n",
    "                if not re.match(\"[A-Z]+\", word):\n",
    "                    continue\n",
    "                if word not in features[index]:\n",
    "                    features[index][word] = True\n",
    "\n",
    "    max_categories = 0\n",
    "    for keys in [f.keys() for f in features]:\n",
    "        max_categories = max(max_categories, len(keys))\n",
    "\n",
    "    features = [f.keys() for f in features]\n",
    "    \n",
    "    word_dim = WORD_DIM + 4  # We add the capital features initially\n",
    "    for feature in features[:-1]:\n",
    "        word_dim += len(feature)\n",
    "    \n",
    "    targets = len(features[-1])\n",
    "    \n",
    "    return num_features, max_categories, max_length, word_dim, targets, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Dimension: 300            The size of the original word embeddings\n",
      "MAX_SENTENCE_LENGTH: 30        The maximum length a sentence is allowed to have\n",
      "FEATURE_AMOUNT: 3              The amount of imput features including the word itself\n",
      "MAX_FEATURE_CATEGORIES: 37     The size of the max amount of categories within one feature (words not included)\n",
      "INPUT EMBEDDING LENGTH: 358    The final size of the embedding (one-hot feature embbedings + word embedding)\n",
      "NUM_TARGETS: 8                 The number of possible targets that the model should predict\n",
      "\n",
      "\n",
      "Available Extra Word Features\n",
      "[u'CC', u'CD', u'DT', u'EX', u'FW', u'IN', u'JJ', u'JJR', u'JJS', u'LS', u'MD', u'NN', u'NNP', u'NNPS', u'NNS', u'NN|SYM', u'PDT', u'POS', u'PRP', u'PRP$', u'RB', u'RBR', u'RBS', u'RP', u'SYM', u'TO', u'UH', u'VB', u'VBD', u'VBG', u'VBN', u'VBP', u'VBZ', u'WDT', u'WP', u'WP$', u'WRB']\n",
      "\n",
      "[u'B-ADJP', u'B-ADVP', u'B-NP', u'B-PP', u'B-SBAR', u'B-VP', u'I-ADJP', u'I-ADVP', u'I-CONJP', u'I-INTJ', u'I-LST', u'I-NP', u'I-PP', u'I-PRT', u'I-SBAR', u'I-VP', u'O']\n",
      "\n",
      "\n",
      "Available Targets\n",
      "[u'O', u'B-MISC', u'B-ORG', u'I-LOC', u'I-MISC', u'I-ORG', u'I-PER', u'B-LOC'] \n",
      "\n",
      "Location -> Target\n",
      "{u'B-LOC': 7,\n",
      " u'B-MISC': 1,\n",
      " u'B-ORG': 2,\n",
      " u'I-LOC': 3,\n",
      " u'I-MISC': 4,\n",
      " u'I-ORG': 5,\n",
      " u'I-PER': 6,\n",
      " u'O': 0}\n",
      "\n",
      "Target -> Location\n",
      "{0: u'O',\n",
      " 1: u'B-MISC',\n",
      " 2: u'B-ORG',\n",
      " 3: u'I-LOC',\n",
      " 4: u'I-MISC',\n",
      " 5: u'I-ORG',\n",
      " 6: u'I-PER',\n",
      " 7: u'B-LOC'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define All data parameters\n",
    "\"\"\"\n",
    "\n",
    "WORD_DIM = 300\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "\n",
    "TARGET_LANGUAGE=\"eng\" # The language one wishes to train for\n",
    "DATA_DIR=\"data\" # path to coNLL data set\n",
    "\n",
    "\n",
    "TRAINING_FILE=\"{}/{}.train\".format(DATA_DIR, TARGET_LANGUAGE)\n",
    "DEV_FILE=\"{}/{}.testa\".format(DATA_DIR, TARGET_LANGUAGE)\n",
    "VALIDATION_FILE=\"{}/{}.testb\".format(DATA_DIR, TARGET_LANGUAGE)\n",
    "\n",
    "EMBEDDING_DIR=\"embeddings\" # path to the location of the word embeddings\n",
    "EMBEDDING_FILE=\"{}/{}.bin\".format(EMBEDDING_DIR, TARGET_LANGUAGE)\n",
    "\n",
    "FEATURE_AMOUNT, \\\n",
    "MAX_FEATURE_CATEGORIES, \\\n",
    "MAX_SENTENCE_LENGTH, \\\n",
    "EMBEDDING_LENGTH, \\\n",
    "NUM_TARGETS, \\\n",
    "FEATURES = analyse_training_file(TRAINING_FILE)\n",
    "\n",
    "\"\"\"\n",
    "Sort the targets to ensure that the predictions vs. the targets is in order and can be reasoned about\n",
    "\"\"\"\n",
    "for feature in FEATURES:\n",
    "    feature.sort()\n",
    "\n",
    "print(\"\"\"\n",
    "Word Dimension: {word_dim}            The size of the original word embeddings\n",
    "MAX_SENTENCE_LENGTH: {sent_len}        The maximum length a sentence is allowed to have\n",
    "FEATURE_AMOUNT: {feat_am}              The amount of imput features including the word itself\n",
    "MAX_FEATURE_CATEGORIES: {max_feat_cat}     The size of the max amount of categories within one feature (words not included)\n",
    "INPUT EMBEDDING LENGTH: {emb_len}    The final size of the embedding (one-hot feature embbedings + word embedding)\n",
    "NUM_TARGETS: {num_tar}                 The number of possible targets that the model should predict\n",
    "\"\"\".format(word_dim=WORD_DIM, \n",
    "           sent_len=MAX_SENTENCE_LENGTH, \n",
    "           feat_am=FEATURE_AMOUNT, \n",
    "           max_feat_cat=MAX_FEATURE_CATEGORIES,\n",
    "           emb_len=EMBEDDING_LENGTH,\n",
    "           num_tar=NUM_TARGETS))\n",
    "\n",
    "print(\"\\nAvailable Extra Word Features\")\n",
    "for feature in FEATURES[:-1]:\n",
    "    print(\"{}\\n\".format(feature))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Ensure that O (The Non Named Entity) is the first element in the feature list\n",
    "\"\"\"\n",
    "o_index = FEATURES[-1].index('O')\n",
    "first_element = FEATURES[-1][0]\n",
    "FEATURES[-1][0] = FEATURES[-1][o_index]\n",
    "FEATURES[-1][o_index] = first_element\n",
    "\n",
    "\n",
    "print(\"\\nAvailable Targets\")\n",
    "print(FEATURES[-1], \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Create a dictionary to be able to convert the onehot vectors back to their original label\n",
    "\"\"\"\n",
    "TARGET_LOCATION = {}\n",
    "LOCATION_TARGETS = {}\n",
    "for target in FEATURES[-1]:\n",
    "    onehot = np.zeros(len(FEATURES[-1]))\n",
    "    onehot[FEATURES[-1].index(target)] = 1\n",
    "    LOCATION_TARGETS[np.argmax(onehot)] = target\n",
    "    TARGET_LOCATION[target] = np.argmax(onehot)\n",
    "\n",
    "\n",
    "print(\"Location -> Target\")\n",
    "pprint(TARGET_LOCATION)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Target -> Location\")\n",
    "pprint(LOCATION_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary from the files\n",
      "Loading in the pretrained language model\n",
      "Generating wordvectors for the entire vocabulary\n",
      "\n",
      "Done!\n",
      "\n",
      "Vocabulary Size: 30290\n",
      "Word Vectors: 30290\n",
      "Pretrained Vectors: 21917 (72.3572136018%)\n",
      "Random Vectors: 8373 (27.6427863982%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create word_vectors \n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating the vocabulary from the files\")\n",
    "vocabulary = {}\n",
    "for _file in [TRAINING_FILE, DEV_FILE, VALIDATION_FILE]:\n",
    "    for line in codecs.open(_file, \"r\", encoding=\"utf-8\").readlines():\n",
    "        if line in ['\\n', '\\r\\n']:\n",
    "            continue\n",
    "        word = line.split()[0].strip()\n",
    "        if not word in vocabulary:\n",
    "            vocabulary[word] = None\n",
    "\n",
    "print(\"Loading in the pretrained language model\")\n",
    "try:\n",
    "    word_embeddings_file = Word2Vec.load_word2vec_format(EMBEDDING_FILE, binary=True)  # C binary format\n",
    "except:\n",
    "    print(\"\\nThere was an error loading the pretrained vectors. All vectors will be random\")\n",
    "    print(\"To use pretrained word vectors please provide a vector file in location: {}\\n\".format(EMBEDDING_FILE))\n",
    "    word_embeddings_file = None\n",
    "    \n",
    "print(\"Generating wordvectors for the entire vocabulary\")\n",
    "pretrained = 0\n",
    "random_amount = 0\n",
    "word_vectors = dict()\n",
    "for word in vocabulary.keys():\n",
    "    if word not in word_vectors:\n",
    "        try:\n",
    "            word_vectors[word] = word_embeddings_file[word]  # raw numpy vector of a word given it exists in the model\n",
    "            pretrained += 1\n",
    "        except:\n",
    "            word_vectors[word] = np.random.uniform(-0.25,0.25,WORD_DIM) # Random numpy vector\n",
    "            random_amount += 1\n",
    "\n",
    "print (\"\"\"\n",
    "Done!\n",
    "\n",
    "Vocabulary Size: {vocab}\n",
    "Word Vectors: {words}\n",
    "Pretrained Vectors: {pre_vec} ({pre_perc}%)\n",
    "Random Vectors: {ran_vec} ({ran_perc}%)\n",
    "\n",
    "\"\"\".format(vocab=len(vocabulary),\n",
    "           words=len(word_vectors),\n",
    "           pre_vec=pretrained,\n",
    "           pre_perc=float(pretrained) / len(word_vectors) * 100,\n",
    "           ran_vec=random_amount,\n",
    "           ran_perc=float(random_amount) / len(word_vectors) * 100\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the methods used to convert the other features to vectors\n",
    "\"\"\"\n",
    "def get_feature_vector(category, feature_index):\n",
    "    onehot = np.zeros(len(FEATURES[feature_index]))\n",
    "    # Assign one element in the vector to one, corresponding to the index\n",
    "    # of the category in features\n",
    "    try:\n",
    "        onehot[FEATURES[feature_index].index(category)] = 1\n",
    "    except:\n",
    "        pass\n",
    "    return onehot\n",
    "\n",
    "def cap_feature(word):\n",
    "    \"\"\"\n",
    "    Capitalization feature:\n",
    "    0 = low caps\n",
    "    1 = all caps\n",
    "    2 = first letter caps\n",
    "    3 = one capital (not first letter)\n",
    "    \"\"\"\n",
    "    if word.lower() == word:\n",
    "        return np.array([1, 0 ,0, 0])\n",
    "    elif word.upper() == word:\n",
    "        return np.array([0, 1 ,0, 0])\n",
    "    elif word[0].upper() == word[0]:\n",
    "        return np.array([0, 0 ,1, 0])\n",
    "    else:\n",
    "        return np.array([0, 0 ,0, 1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating input and target vectors for the dataset\n",
      "\n",
      "    Train Data\n",
      "\n",
      "    Train Sentences: 16779\n",
      "    Sentence Length: 30\n",
      "    Embedding Length: 358\n",
      "    Input Shape: (30, 358)\n",
      "\n",
      "    Number of Targets: 16779\n",
      "    Target Sentences: 30\n",
      "    Target Length: 8\n",
      "    Target Shape: (30, 8)\n",
      "    \n",
      "\n",
      "    Development Data\n",
      "\n",
      "    Train Sentences: 3972\n",
      "    Sentence Length: 30\n",
      "    Embedding Length: 358\n",
      "    Input Shape: (30, 358)\n",
      "\n",
      "    Number of Targets: 3972\n",
      "    Target Sentences: 30\n",
      "    Target Length: 8\n",
      "    Target Shape: (30, 8)\n",
      "    \n",
      "\n",
      "    Validation Data\n",
      "\n",
      "    Train Sentences: 4072\n",
      "    Sentence Length: 30\n",
      "    Embedding Length: 358\n",
      "    Input Shape: (30, 358)\n",
      "\n",
      "    Number of Targets: 4072\n",
      "    Target Sentences: 30\n",
      "    Target Length: 8\n",
      "    Target Shape: (30, 8)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def create_input_vectors(file_name):\n",
    "    words = []\n",
    "    features = []\n",
    "    sentences = []\n",
    "    sentence_features = []\n",
    "    sentence_length = MAX_SENTENCE_LENGTH\n",
    "    current_sentence_length = 0\n",
    "\n",
    "    lines = codecs.open(file_name, encoding=\"utf-8\").readlines()\n",
    "    lines = [l for l in lines if \"-DOCSTART-\" not in l]\n",
    "\n",
    "    for line in lines:\n",
    "        if line in ['\\n', '\\r\\n']:\n",
    "            # end of line. Make sure all sentences are of equal length\n",
    "            for _ in range(sentence_length - current_sentence_length):\n",
    "                words.append(np.zeros(EMBEDDING_LENGTH))\n",
    "                features.append(np.zeros(len(FEATURES[-1])))\n",
    "\n",
    "            # Add current sentence words to sentences and refresh the lists\n",
    "            sentences.append(words)\n",
    "            sentence_features.append(features)\n",
    "            words = []\n",
    "            features = []\n",
    "            current_sentence_length = 0\n",
    "        else:\n",
    "            # Make sure all lines have the right amount of features\n",
    "            assert(len(line.split()) == FEATURE_AMOUNT + 1)\n",
    "\n",
    "            # make sure no sentence is longer than max_sentence_length\n",
    "            if current_sentence_length == sentence_length:\n",
    "                sentences.append(words)\n",
    "                sentence_features.append(features)\n",
    "                words = []\n",
    "                features = []\n",
    "                current_sentence_length = 0\n",
    "\n",
    "            # get the vector of the word in first position of each line\n",
    "            word_and_features = line.split()\n",
    "            temp = []\n",
    "            temp = np.append(temp, word_vectors[word_and_features[0]])\n",
    "\n",
    "            # get the feature vector for each feature of the word\n",
    "            for index, feature in enumerate(word_and_features[1:-1]):\n",
    "                temp = np.append(temp, get_feature_vector(feature, index))\n",
    "            \"\"\"\n",
    "            Below are some additional features\n",
    "            \"\"\"\n",
    "\n",
    "            temp = np.append(temp, cap_feature(word_and_features[0]))\n",
    "            words.append(temp)\n",
    "\n",
    "            # Add the tag to the tag list\n",
    "            features.append(get_feature_vector(word_and_features[-1], len(FEATURES) - 1))\n",
    "\n",
    "\n",
    "            current_sentence_length += 1\n",
    "\n",
    "\n",
    "    # Check there are features for each sentence\n",
    "    assert(len(sentences) == len(sentence_features))\n",
    "    return np.asarray(sentences), np.array(sentence_features)\n",
    "\n",
    "print(\"Creating input and target vectors for the dataset\")\n",
    "pool = Pool(processes=3)\n",
    "train_process = pool.apply_async(create_input_vectors, args=(TRAINING_FILE,))\n",
    "dev_process = pool.apply_async(create_input_vectors, args=(DEV_FILE,))\n",
    "val_process = pool.apply_async(create_input_vectors, args=(VALIDATION_FILE,))\n",
    "\n",
    "train_input, train_target = train_process.get()\n",
    "dev_input, dev_target = dev_process.get()\n",
    "validation_input, validation_target = val_process.get()\n",
    "\n",
    "\n",
    "for name, data_file, target_file in [(\"Train\", train_input, train_target), \n",
    "                                     (\"Development\", dev_input, dev_target),\n",
    "                                     (\"Validation\", validation_input, validation_target)]:\n",
    "    \n",
    "    print(\"\"\"\n",
    "    {name} Data\n",
    "\n",
    "    Train Sentences: {input_amount}\n",
    "    Sentence Length: {input_len}\n",
    "    Embedding Length: {emb_size}\n",
    "    Input Shape: ({input_len}, {emb_size})\n",
    "\n",
    "    Number of Targets: {target_amount}\n",
    "    Target Sentences: {target_len}\n",
    "    Target Length: {target_size}\n",
    "    Target Shape: ({target_len}, {target_size})\n",
    "    \"\"\".format(name=name,\n",
    "               input_amount=len(data_file),\n",
    "               input_len=len(data_file[0]),\n",
    "               emb_size=len(data_file[1][2]),\n",
    "               target_amount=len(target_file),\n",
    "               target_len=len(target_file[0]),\n",
    "               target_size=len(target_file[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_target[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create functions used to keep track of the training and get insight into how the performance is developing\n",
    "\"\"\"\n",
    "\n",
    "                              \n",
    "def generate_confusion_matrix(prediction, target, epoch):\n",
    "    def generate_prediction_matrix(predictions, targets):\n",
    "        matrix = np.zeros((len(TARGET_VECTORS), len(TARGET_VECTORS)), dtype=np.float32)\n",
    "        order = {}\n",
    "\n",
    "        target_names = TARGET_VECTORS.values()\n",
    "        target_names.sort()\n",
    "        for sent_index, sentence in enumerate(predictions):\n",
    "            for index, pred in enumerate(sentence):\n",
    "                try:\n",
    "                    predicted_target = TARGET_VECTORS[str(pred)]\n",
    "                    true_target = TARGET_VECTORS[str(targets[sent_index][index])]\n",
    "\n",
    "                    row = target_names.index(predicted_target)\n",
    "                    col = target_names.index(true_target)\n",
    "                    matrix[row][col] = 1\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        target_names_with_count = []\n",
    "        for i in range(len(matrix)):\n",
    "            row_sum = sum(matrix[i])\n",
    "            target_names_with_count.append(\"{} ({})\".format(target_names[i], int(row_sum)))\n",
    "            for j in range(len(matrix[i])):\n",
    "                matrix[i][j] = matrix[i][j] / row_sum if row_sum != 0 else 0\n",
    "            \n",
    "        return matrix, target_names_with_count, target_names\n",
    "    \n",
    "    matrix, rows, columns = generate_prediction_matrix(prediction, target)\n",
    "    df_cm = pd.DataFrame(matrix, \n",
    "                         index = [i for i in rows],\n",
    "                         columns = [i for i in columns])\n",
    "\n",
    "    title = \"Epoch: {}\".format(epoch)\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    confusion = sn.heatmap(df_cm, annot=True)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.savefig(\"confusion_matrix.png\")    \n",
    "\n",
    "def cost(prediction, target):\n",
    "    target = tf.reshape(target, [-1, MAX_SENTENCE_LENGTH, NUM_TARGETS])\n",
    "    prediction = tf.reshape(prediction, [-1, MAX_SENTENCE_LENGTH, NUM_TARGETS])\n",
    "    \n",
    "    cross_entropy = target * tf.log(prediction)\n",
    "    cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "    \n",
    "    mask = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2))\n",
    "    \n",
    "    cross_entropy *= mask\n",
    "    cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "    cross_entropy /= tf.cast(length(target), tf.float32)\n",
    "    return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "def length(target):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def f1(prediction,target): # not tensors but result values\n",
    "    # Reshape the big arrays into smaller sizes with shape (MAX_SENTENCE_LENGTH, NUM_TARGETS)\n",
    "    target = np.reshape(target, (-1, MAX_SENTENCE_LENGTH, NUM_TARGETS))\n",
    "    prediction = np.reshape(prediction, (-1, MAX_SENTENCE_LENGTH, NUM_TARGETS))\n",
    "    \n",
    "   \n",
    "    \n",
    "    true_positive=np.asarray([0]*(NUM_TARGETS+2))\n",
    "    false_positive=np.asarray([0]*(NUM_TARGETS+2))\n",
    "    false_negative=np.asarray([0]*(NUM_TARGETS+2))\n",
    "\n",
    "    target = np.argmax(target, 2)\n",
    "    prediction = np.argmax(prediction, 2)\n",
    "\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        for j in range(MAX_SENTENCE_LENGTH):\n",
    "            if target[i][j] == prediction[i][j]:\n",
    "                true_positive[target[i][j]] += 1\n",
    "            else:\n",
    "                false_positive[target[i][j]] += 1\n",
    "                false_negative[prediction[i][j]] += 1\n",
    "\n",
    "    NON_NAMED_ENTITY = TARGET_LOCATION['O']\n",
    "    for i in range(NUM_TARGETS):\n",
    "        if i != NON_NAMED_ENTITY:\n",
    "            true_positive[-2] += true_positive[i]\n",
    "            false_positive[-2] += false_positive[i]\n",
    "            false_negative[-2] += false_negative[i]\n",
    "        else:\n",
    "            true_positive[-1] += true_positive[i]\n",
    "            false_positive[-1] += false_positive[i]\n",
    "            false_negative[-1] += false_negative[i]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    for i in range(NUM_TARGETS+2):\n",
    "        division_point = true_positive[i]+false_positive[i]\n",
    "        if true_positive[i]+false_positive[i] != 0:\n",
    "            precision.append(true_positive[i]*1.0/true_positive[i]+false_positive[i])\n",
    "        else:\n",
    "            precision.append(0.0)\n",
    "           \n",
    "        \n",
    "        if true_positive[i]+false_negative[i] != 0:\n",
    "            recall.append(true_positive[i]*1.0/true_positive[i]+false_negative[i])\n",
    "        else:\n",
    "            recall.append(0.0)\n",
    "            \n",
    "        if precision[i]+recall[i] != 0:\n",
    "            fscore.append(2.0*precision[i]*recall[i]/(precision[i]+recall[i]))\n",
    "        else:\n",
    "            fscore.append(0.0)\n",
    "\n",
    "    print(\"precision = \" ,precision)\n",
    "    print(\"recall = \" ,recall)\n",
    "    print(\"f1score = \" ,fscore)\n",
    "    efs = fscore[-2]\n",
    "    print(\"Entity fscore :\", efs )\n",
    "    return efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network building\n",
      "Network Built\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define Model Parameters\n",
    "\"\"\"\n",
    "# resetting the graph\n",
    "reset_default_graph()\n",
    "\n",
    "NUM_HIDDEN = 100\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "print(\"Network building\")\n",
    "\n",
    "net = tflearn.input_data([None, MAX_SENTENCE_LENGTH, EMBEDDING_LENGTH])\n",
    "net = rnn.bidirectional_rnn(MultiRNNCell([GRUCell(NUM_HIDDEN)]*NUM_LAYERS), MultiRNNCell([GRUCell(NUM_HIDDEN)]*NUM_LAYERS), tf.unpack(tf.transpose(net, perm=[1, 0, 2])), dtype=tf.float32)  #256=num_hidden, 3=num_layers\n",
    "net = tflearn.dropout(net[0], DROPOUT)\n",
    "net = tf.transpose(tf.pack(net), perm=[1, 0, 2])\n",
    "\n",
    "net = tflearn.fully_connected(net, MAX_SENTENCE_LENGTH*NUM_TARGETS, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam',loss=cost)\n",
    "\n",
    "model = tflearn.DNN(net, clip_gradients=0., tensorboard_verbose=0)\n",
    "print(\"Network Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16779, 240)\n",
      "(3972, 240)\n",
      "(4072, 240)\n"
     ]
    }
   ],
   "source": [
    "train_target = np.asarray(train_target).astype(int).reshape(len(train_target),-1)\n",
    "print(train_target.shape)\n",
    "\n",
    "dev_target = np.asarray(dev_target).astype(int).reshape(len(dev_target),-1)\n",
    "print(dev_target.shape)\n",
    "\n",
    "\n",
    "validation_target = np.asarray(validation_target).astype(int).reshape(len(validation_target),-1)\n",
    "print(validation_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 505  | total loss: \u001b[1m\u001b[32mnan\u001b[0m\u001b[0m\n",
      "\u001b[2K\r",
      "| Adam | epoch: 000 | loss: nan -- iter: 11008/16779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-72fe92b829f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m                          \u001b[0mdaug_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaug_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                          run_id=run_id)\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    296\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_global_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                         termlogger.on_batch_end(global_loss, global_acc,\n\u001b[0;32m--> 298\u001b[0;31m                                                 snapshot)\n\u001b[0m\u001b[1;32m    299\u001b[0m                         \u001b[0mmodelsaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, global_loss, global_acc, snapshot)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_termlogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_termlogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36mprint_termlogs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mtermlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermlogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_ipython\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_curses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36mclear_output\u001b[0;34m(wait)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractiveshell\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\033[2K\\r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.pyc\u001b[0m in \u001b[0;36mclear_output\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    126\u001b[0m         self.session.send(\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'clear_output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/jupyter_client/session.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0;31m# use dummy tracker, which will be done immediately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_send\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36msend_multipart\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;34m\"\"\"Schedule send in IO thread\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36msend_multipart\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_really_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_really_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/tornado/ioloop.pyc\u001b[0m in \u001b[0;36madd_callback\u001b[0;34m(self, callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mlist_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                 self._callbacks.append(functools.partial(\n\u001b[0;32m--> 941\u001b[0;31m                     stack_context.wrap(callback), *args, **kwargs))\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlist_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0;31m# If we're not in the IOLoop's thread, and we added the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vilstrup/anaconda2/lib/python2.7/site-packages/tornado/stack_context.pyc\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \"\"\"\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Check if function is already wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_wrapped'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define Training Parameters\n",
    "\"\"\"\n",
    "BATCH_SIZE = 256\n",
    "BATCH_AMOUNT = (len(train_input) + BATCH_SIZE - 1) / BATCH_SIZE\n",
    "NUM_EPOCH = 200\n",
    "\n",
    "print(\"Batches pr. Epoch: {}\\n\".format(BATCH_AMOUNT))\n",
    "\n",
    "max_entity = 0\n",
    "max_batch = 0\n",
    "batch = 0\n",
    "reached_max = 0\n",
    "\n",
    "while True:\n",
    "    with ops.get_default_graph().as_default():\n",
    "        model.fit(train_input, train_target,n_epoch=1, validation_set=(dev_input,dev_target), show_metric=False, batch_size=BATCH_SIZE)\n",
    "        batch += 75\n",
    "        val_pred =np.asarray(model.predict(validation_input))\n",
    "        entity_f1 = f1(val_pred, validation_target)\n",
    "        former_max = max_entity\n",
    "\n",
    "        if entity_f1 >= max_entity:\n",
    "            max_entity = entity_f1\n",
    "            max_batch = batch\n",
    "            reached_max = 0\n",
    "\n",
    "        if entity_f1 - former_max < 1:\n",
    "            reached_max += 1\n",
    "\n",
    "        print(\"max entity f1: {}, batch nr: {}, early stop counter: {}\".format(max_entity, max_batch, reached_max))\n",
    "        if reached_max == 10:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
