{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import all the necessary packages\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "import random\n",
    "import argparse\n",
    "from multiprocessing import Pool, Process, cpu_count\n",
    "import random\n",
    "import pickle as pkl\n",
    "import os\n",
    "import codecs\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import re\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_training_file(training_file):\n",
    "    \"\"\"\n",
    "    Analyse the amount of features in the training dataset. The data should\n",
    "    look like the following:\n",
    "\n",
    "    Mikkel feat1 feat2 ... B-PERS\n",
    "    Vilstrup feat1 feat2 ... I-PERS\n",
    "    is feat1 feat2 ... O\n",
    "    the feat1 feat2 ... O\n",
    "    author feat1 feat2 ... O\n",
    "    ...\n",
    "\n",
    "    where feat1 feat2 ... are features related to the word on the left\n",
    "    It is assumed that the features never change position, and that each feature\n",
    "    is seperated by a space (\" \").\n",
    "    \"\"\"\n",
    "    lines = codecs.open(training_file, encoding=\"utf-8\").readlines()\n",
    "    lines = [l for l in lines if \"-DOCSTART-\" not in l]\n",
    "\n",
    "    num_features = 0\n",
    "    for line in lines:\n",
    "        num_features = max(num_features, len(line.split()) - 1) # find the maximum amount of features\n",
    "\n",
    "    features = [{} for i in range(num_features)] # create a dictionary for each feature\n",
    "\n",
    "    max_length = 0\n",
    "    current_length = 0\n",
    "    # Iterate through all the lines to get the categories of each feature\n",
    "    for line in lines:\n",
    "        if line in ['\\n', '\\r\\n']:\n",
    "            # this is the end of a sentence.\n",
    "            max_length = min(MAX_SENTENCE_LENGTH, max(max_length, current_length))\n",
    "            current_length = 0\n",
    "            continue\n",
    "        else:\n",
    "            current_length +=1\n",
    "            words = line.split()[1:] # discard the word on the left\n",
    "\n",
    "            for index, word in enumerate(words):\n",
    "                if not re.match(\"[A-Z]+\", word):\n",
    "                    continue\n",
    "                if word not in features[index]:\n",
    "                    features[index][word] = True\n",
    "\n",
    "    max_categories = 0\n",
    "    for keys in [f.keys() for f in features]:\n",
    "        max_categories = max(max_categories, len(keys))\n",
    "\n",
    "    features = [f.keys() for f in features]\n",
    "    \n",
    "    word_dim = WORD_DIM + 4  # We add the capital features initially\n",
    "    for feature in features[:-1]:\n",
    "        word_dim += len(feature)\n",
    "    \n",
    "    targets = len(features[-1])\n",
    "    \n",
    "    return num_features, max_categories, max_length, word_dim, targets, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Dimension: 300            The size of the original word embeddings\n",
      "MAX_SENTENCE_LENGTH: 30        The maximum length a sentence is allowed to have\n",
      "FEATURE_AMOUNT: 3              The amount of imput features including the word itself\n",
      "MAX_FEATURE_CATEGORIES: 37     The size of the max amount of categories within one feature (words not included)\n",
      "INPUT EMBEDDING LENGTH: 358    The final size of the embedding (one-hot feature embbedings + word embedding)\n",
      "NUM_TARGETS: 8                 The number of possible targets that the model should predict\n",
      "\n",
      "\n",
      "Available Extra Word Features\n",
      "[u'PRP$', u'VBG', u'VBD', u'VBN', u'VBP', u'WDT', u'JJ', u'WP', u'VBZ', u'DT', u'RP', u'NN', u'FW', u'POS', u'TO', u'PRP', u'RB', u'NNS', u'NNP', u'VB', u'WRB', u'CC', u'LS', u'PDT', u'RBS', u'RBR', u'CD', u'EX', u'IN', u'WP$', u'NN|SYM', u'MD', u'NNPS', u'JJS', u'JJR', u'SYM', u'UH']\n",
      "\n",
      "[u'I-PP', u'I-CONJP', u'B-ADJP', u'I-ADVP', u'B-VP', u'I-VP', u'B-PP', u'I-SBAR', u'O', u'I-INTJ', u'I-NP', u'I-ADJP', u'B-SBAR', u'B-ADVP', u'B-NP', u'I-LST', u'I-PRT']\n",
      "\n",
      "\n",
      "Available Targets\n",
      "[u'I-LOC', u'B-ORG', u'O', u'I-PER', u'I-MISC', u'B-MISC', u'I-ORG', u'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define All data parameters\n",
    "\"\"\"\n",
    "\n",
    "WORD_DIM = 300\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "MAX_WORD_LENGTH=15\n",
    "\n",
    "\n",
    "DATA_DIR=\"data\" # path to coNLL data set\n",
    "TARGET_LANGUAGE=\"eng\" # The language one wishes to train for\n",
    "\n",
    "TRAINING_FILE=\"{}/{}.train\".format(DATA_DIR, TARGET_LANGUAGE)\n",
    "DEV_FILE=\"{}/{}.testa\".format(DATA_DIR, TARGET_LANGUAGE)\n",
    "VALIDATION_FILE=\"{}/{}.testb\".format(DATA_DIR, TARGET_LANGUAGE)\n",
    "\n",
    "EMBEDDING_DIR=\"embeddings\" # path to the location of the word embeddings\n",
    "EMBEDDING_FILE=\"{}/{}.bin\".format(EMBEDDING_DIR, TARGET_LANGUAGE)\n",
    "\n",
    "FEATURE_AMOUNT, \\\n",
    "MAX_FEATURE_CATEGORIES, \\\n",
    "MAX_SENTENCE_LENGTH, \\\n",
    "EMBEDDING_LENGTH, \\\n",
    "NUM_TARGETS, \\\n",
    "FEATURES = analyse_training_file(TRAINING_FILE)\n",
    "\n",
    "print \\\n",
    "\"\"\"\n",
    "Word Dimension: {word_dim}            The size of the original word embeddings\n",
    "MAX_SENTENCE_LENGTH: {sent_len}        The maximum length a sentence is allowed to have\n",
    "FEATURE_AMOUNT: {feat_am}              The amount of imput features including the word itself\n",
    "MAX_FEATURE_CATEGORIES: {max_feat_cat}     The size of the max amount of categories within one feature (words not included)\n",
    "INPUT EMBEDDING LENGTH: {emb_len}    The final size of the embedding (one-hot feature embbedings + word embedding)\n",
    "NUM_TARGETS: {num_tar}                 The number of possible targets that the model should predict\n",
    "\"\"\".format(word_dim=WORD_DIM, \n",
    "           sent_len=MAX_SENTENCE_LENGTH, \n",
    "           feat_am=FEATURE_AMOUNT, \n",
    "           max_feat_cat=MAX_FEATURE_CATEGORIES,\n",
    "           emb_len=EMBEDDING_LENGTH,\n",
    "           num_tar=NUM_TARGETS)\n",
    "\n",
    "print(\"\\nAvailable Extra Word Features\")\n",
    "for feature in FEATURES[:-1]:\n",
    "    print(\"{}\\n\".format(feature))\n",
    "    \n",
    "print(\"\\nAvailable Targets\")\n",
    "print(FEATURES[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary from the files\n",
      "Loading in the pretrained language model\n",
      "Generating wordvectors for the entire vocabulary\n",
      "\n",
      "Done!\n",
      "\n",
      "Vocabulary Size: 30290\n",
      "Word Vectors: 30290\n",
      "Pretrained Vectors: 21917 (72.3572136018%)\n",
      "Random Vectors: 8373 (27.6427863982%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create word_vectors \n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating the vocabulary from the files\")\n",
    "vocabulary = {}\n",
    "for _file in [TRAINING_FILE, DEV_FILE, VALIDATION_FILE]:\n",
    "    for line in codecs.open(_file, \"r\", encoding=\"utf-8\").readlines():\n",
    "        if line in ['\\n', '\\r\\n']:\n",
    "            continue\n",
    "        word = line.split()[0].strip()\n",
    "        if not word in vocabulary:\n",
    "            vocabulary[word] = None\n",
    "\n",
    "print(\"Loading in the pretrained language model\")\n",
    "try:\n",
    "    word_embeddings_file = Word2Vec.load_word2vec_format(EMBEDDING_FILE, binary=True)  # C binary format\n",
    "except:\n",
    "    print(\"\\nThere was an error loading the pretrained vectors. All vectors will be random\")\n",
    "    print(\"To use pretrained word vectors please provide a vector file in location: {}\\n\".format(EMBEDDING_FILE))\n",
    "    word_embeddings_file = None\n",
    "    \n",
    "print(\"Generating wordvectors for the entire vocabulary\")\n",
    "pretrained = 0\n",
    "random_amount = 0\n",
    "word_vectors = dict()\n",
    "for word in vocabulary.keys():\n",
    "    if word not in word_vectors:\n",
    "        try:\n",
    "            word_vectors[word] = word_embeddings_file[word]  # raw numpy vector of a word given it exists in the model\n",
    "            pretrained += 1\n",
    "        except:\n",
    "            word_vectors[word] = np.random.uniform(-0.25,0.25,WORD_DIM) # Random numpy vector\n",
    "            random_amount += 1\n",
    "\n",
    "print \\\n",
    "\"\"\"\n",
    "Done!\n",
    "\n",
    "Vocabulary Size: {vocab}\n",
    "Word Vectors: {words}\n",
    "Pretrained Vectors: {pre_vec} ({pre_perc}%)\n",
    "Random Vectors: {ran_vec} ({ran_perc}%)\n",
    "\n",
    "\"\"\".format(vocab=len(vocabulary),\n",
    "           words=len(word_vectors),\n",
    "           pre_vec=pretrained,\n",
    "           pre_perc=float(pretrained) / len(word_vectors) * 100,\n",
    "           ran_vec=random_amount,\n",
    "           ran_perc=float(random_amount) / len(word_vectors) * 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[ 0.  0.  0.  0.  0.  0.  0.  1.]': u'O',\n",
      " '[ 0.  0.  0.  0.  0.  0.  1.  0.]': u'I-PER',\n",
      " '[ 0.  0.  0.  0.  0.  1.  0.  0.]': u'I-ORG',\n",
      " '[ 0.  0.  0.  0.  1.  0.  0.  0.]': u'I-MISC',\n",
      " '[ 0.  0.  0.  1.  0.  0.  0.  0.]': u'I-LOC',\n",
      " '[ 0.  0.  1.  0.  0.  0.  0.  0.]': u'B-ORG',\n",
      " '[ 0.  1.  0.  0.  0.  0.  0.  0.]': u'B-MISC',\n",
      " '[ 1.  0.  0.  0.  0.  0.  0.  0.]': u'B-LOC'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define the methods used to convert the other features to vectors\n",
    "\"\"\"\n",
    "def get_feature_vector(category, feature_index):\n",
    "    onehot = np.zeros(len(FEATURES[feature_index]))\n",
    "    # Assign one element in the vector to one, corresponding to the index\n",
    "    # of the category in features\n",
    "    try:\n",
    "        onehot[FEATURES[feature_index].index(category)] = 1\n",
    "    except:\n",
    "        pass\n",
    "    return onehot\n",
    "\n",
    "def cap_feature(word):\n",
    "    \"\"\"\n",
    "    Capitalization feature:\n",
    "    0 = low caps\n",
    "    1 = all caps\n",
    "    2 = first letter caps\n",
    "    3 = one capital (not first letter)\n",
    "    \"\"\"\n",
    "    if word.lower() == word:\n",
    "        return np.array([1, 0 ,0, 0])\n",
    "    elif word.upper() == word:\n",
    "        return np.array([0, 1 ,0, 0])\n",
    "    elif word[0].upper() == word[0]:\n",
    "        return np.array([0, 0 ,1, 0])\n",
    "    else:\n",
    "        return np.array([0, 0 ,0, 1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating input and target vectors for the dataset\n",
      "\n",
      "    Train Data\n",
      "\n",
      "    Train Sentences: 16779\n",
      "    Sentence Length: 30\n",
      "    Embedding Length: 358\n",
      "    Input Shape: (30, 358)\n",
      "\n",
      "    Number of Targets: 16779\n",
      "    Target Sentences: 30\n",
      "    Target Length: 8\n",
      "    Target Shape: (30, 8)\n",
      "    \n",
      "\n",
      "    Development Data\n",
      "\n",
      "    Train Sentences: 3972\n",
      "    Sentence Length: 30\n",
      "    Embedding Length: 358\n",
      "    Input Shape: (30, 358)\n",
      "\n",
      "    Number of Targets: 3972\n",
      "    Target Sentences: 30\n",
      "    Target Length: 8\n",
      "    Target Shape: (30, 8)\n",
      "    \n",
      "\n",
      "    Validation Data\n",
      "\n",
      "    Train Sentences: 4072\n",
      "    Sentence Length: 30\n",
      "    Embedding Length: 358\n",
      "    Input Shape: (30, 358)\n",
      "\n",
      "    Number of Targets: 4072\n",
      "    Target Sentences: 30\n",
      "    Target Length: 8\n",
      "    Target Shape: (30, 8)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def create_input_vectors(file_name):\n",
    "    words = []\n",
    "    features = []\n",
    "    sentences = []\n",
    "    sentence_features = []\n",
    "    sentence_length = MAX_SENTENCE_LENGTH\n",
    "    current_sentence_length = 0\n",
    "\n",
    "    lines = codecs.open(file_name, encoding=\"utf-8\").readlines()\n",
    "    lines = [l for l in lines if \"-DOCSTART-\" not in l]\n",
    "\n",
    "    for line in lines:\n",
    "        if line in ['\\n', '\\r\\n']:\n",
    "            # end of line. Make sure all sentences are of equal length\n",
    "            for _ in range(sentence_length - current_sentence_length):\n",
    "                words.append(np.zeros(EMBEDDING_LENGTH))\n",
    "                features.append(np.zeros(len(FEATURES[-1])))\n",
    "\n",
    "            # Add current sentence words to sentences and refresh the lists\n",
    "            sentences.append(words)\n",
    "            sentence_features.append(features)\n",
    "            words = []\n",
    "            features = []\n",
    "            current_sentence_length = 0\n",
    "        else:\n",
    "            # Make sure all lines have the right amount of features\n",
    "            assert(len(line.split()) == FEATURE_AMOUNT + 1)\n",
    "\n",
    "            # make sure no sentence is longer than max_sentence_length\n",
    "            if current_sentence_length == sentence_length:\n",
    "                sentences.append(words)\n",
    "                sentence_features.append(features)\n",
    "                words = []\n",
    "                features = []\n",
    "                current_sentence_length = 0\n",
    "\n",
    "            # get the vector of the word in first position of each line\n",
    "            word_and_features = line.split()\n",
    "            temp = []\n",
    "            temp = np.append(temp, word_vectors[word_and_features[0]])\n",
    "\n",
    "            # get the feature vector for each feature of the word\n",
    "            for index, feature in enumerate(word_and_features[1:-1]):\n",
    "                temp = np.append(temp, get_feature_vector(feature, index))\n",
    "            \"\"\"\n",
    "            Below are some additional features\n",
    "            \"\"\"\n",
    "\n",
    "            temp = np.append(temp, cap_feature(word_and_features[0]))\n",
    "            words.append(temp)\n",
    "\n",
    "            # Add the tag to the tag list\n",
    "            features.append(get_feature_vector(word_and_features[-1], len(FEATURES) - 1))\n",
    "\n",
    "\n",
    "            current_sentence_length += 1\n",
    "\n",
    "\n",
    "    # Check there are features for each sentence\n",
    "    assert(len(sentences) == len(sentence_features))\n",
    "    return np.array(sentences), np.array(sentence_features)\n",
    "\n",
    "print(\"Creating input and target vectors for the dataset\")\n",
    "pool = Pool(processes=3)\n",
    "train_process = pool.apply_async(create_input_vectors, args=(TRAINING_FILE,))\n",
    "dev_process = pool.apply_async(create_input_vectors, args=(DEV_FILE,))\n",
    "val_process = pool.apply_async(create_input_vectors, args=(VALIDATION_FILE,))\n",
    "\n",
    "train_input, train_target = train_process.get()\n",
    "dev_input, dev_target = dev_process.get()\n",
    "validation_input, validation_target = val_process.get()\n",
    "\n",
    "\n",
    "for name, data_file, target_file in [(\"Train\", train_input, train_target), \n",
    "                                     (\"Development\", dev_input, dev_target),\n",
    "                                     (\"Validation\", validation_input, validation_target)]:\n",
    "    \n",
    "    print \\\n",
    "    \"\"\"\n",
    "    {name} Data\n",
    "\n",
    "    Train Sentences: {input_amount}\n",
    "    Sentence Length: {input_len}\n",
    "    Embedding Length: {emb_size}\n",
    "    Input Shape: ({input_len}, {emb_size})\n",
    "\n",
    "    Number of Targets: {target_amount}\n",
    "    Target Sentences: {target_len}\n",
    "    Target Length: {target_size}\n",
    "    Target Shape: ({target_len}, {target_size})\n",
    "    \"\"\".format(name=name,\n",
    "               input_amount=len(data_file),\n",
    "               input_len=len(data_file[0]),\n",
    "               emb_size=len(data_file[1][2]),\n",
    "               target_amount=len(target_file),\n",
    "               target_len=len(target_file[0]),\n",
    "               target_size=len(target_file[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the tensorflow model used to train the NER reacogniser\n",
    "\"\"\"\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 target,\n",
    "                 dropout,\n",
    "                 num_hidden, \n",
    "                 num_layers,\n",
    "                 learning_rate):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.dropout = dropout\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        rnn_cell = tf.nn.rnn_cell\n",
    "        # Try: LSTMBlock cell or GruBlock cell\n",
    "        fw_cell = rnn_cell.LSTMCell(self._num_hidden, state_is_tuple=True)\n",
    "        bw_cell = rnn_cell.LSTMCell(self._num_hidden, state_is_tuple=True)\n",
    "\n",
    "        if self._num_layers > 1:\n",
    "            fw_cell = rnn_cell.MultiRNNCell([fw_cell] * self._num_layers, state_is_tuple=True)\n",
    "            fw_cell = rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=self.dropout)\n",
    "            bw_cell = rnn_cell.MultiRNNCell([bw_cell] * self._num_layers, state_is_tuple=True)\n",
    "            bw_cell = rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=self.dropout)\n",
    "        else:\n",
    "            fw_cell = rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=self.dropout)\n",
    "            bw_cell = rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=self.dropout)\n",
    "\n",
    "        # Try: Dynamic Bidirectional RNN\n",
    "        output, _, _ = tf.nn.bidirectional_rnn(fw_cell, \n",
    "                                               bw_cell, \n",
    "                                               tf.unpack(tf.transpose(self.data, perm=[1, 0, 2])), \n",
    "                                               dtype=tf.float32, \n",
    "                                               sequence_length=self.length)\n",
    "\n",
    "\n",
    "        max_length = int(self.target.get_shape()[1])\n",
    "        num_classes = int(self.target.get_shape()[2])\n",
    "        weight, bias = self._weight_and_bias(2*self._num_hidden, num_classes)\n",
    "        output = tf.reshape(tf.transpose(tf.pack(output), perm=[1, 0, 2]), [-1, 2 * self._num_hidden])\n",
    "        prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)\n",
    "        prediction = tf.reshape(prediction, [-1, max_length, num_classes])\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        target = tf.reshape(self.target, [-1, MAX_SENTENCE_LENGTH, NUM_TARGETS])\n",
    "        prediction = tf.reshape(self.prediction, [-1, MAX_SENTENCE_LENGTH, NUM_TARGETS])\n",
    "        cross_entropy = target * tf.log(prediction)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32)\n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    \"\"\"\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = self.target * tf.log(self.prediction)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2) # Summarize the values of 2 axis\n",
    "        \n",
    "        # Check if the maximum value on the secondary axis is positive or negative\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.target), reduction_indices=2)) \n",
    "        cross_entropy *= mask # Ensure the cross_entropy is positive (by multiplying either with -1 or 1)\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1) # Summarize the values on the primary axis\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32) # Convert all dimensions of the vector to 32float.\n",
    "        cost = tf.reduce_mean(cross_entropy) # Reduce the vector to the mean value on all dimensions\n",
    "        \n",
    "        return cost\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 2), tf.argmax(self.prediction, 2))\n",
    "\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.target), reduction_indices=2))\n",
    "        mistakes *= mask\n",
    "        # Average over actual sequence lengths.\n",
    "        mistakes = tf.reduce_sum(mistakes, reduction_indices=1)\n",
    "        mistakes /= tf.cast(self.length, tf.float32)\n",
    "        return tf.reduce_mean(mistakes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @lazy_property\n",
    "    def getpredf1(self):\n",
    "        return self.prediction, self.length\n",
    "\n",
    "print(\"Model Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8a528ea984ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mTARGET_VECTORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mTARGET_VECTORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create functions used to keep track of the training and get insight into how the performance is developing\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Sort the targets to ensure that the predictions vs. the targets is in order and can be reasoned about\n",
    "\"\"\"\n",
    "FEATURES[-1].sort()\n",
    "\n",
    "\"\"\"\n",
    "Create a dictionary to be able to convert the onehot vectors back to their original label\n",
    "\"\"\"\n",
    "TARGET_VECTORS = {}\n",
    "for target in FEATURES[-1]:\n",
    "    onehot = np.zeros(len(FEATURES[-1]))\n",
    "    onehot[FEATURES[-1].index(target)] = 1\n",
    "    TARGET_VECTORS[str(onehot)] = target\n",
    "\n",
    "TARGET_VECTORS[np.zeros(len(FEATURES[-1]))] = \"<pad>\"\n",
    "\n",
    "\n",
    "def generate_confusion_matrix(prediction, target, precision, accuracy, f1, epoch):\n",
    "    def generate_prediction_matrix(predictions, targets):\n",
    "        matrix = np.zeros((len(TARGET_VECTORS), len(TARGET_VECTORS)), dtype=np.float32)\n",
    "        order = {}\n",
    "\n",
    "        target_names = TARGET_VECTORS.values()\n",
    "        target_names.sort()\n",
    "         \n",
    "        for index, pred in enumerate(predictions):\n",
    "            predicted_target = TARGET_VECTORS[str(pred)]\n",
    "            true_target = TARGET_VECTORS[str(targets[index])]\n",
    "            \n",
    "            row = target_names.index(predicted_target)\n",
    "            col = target_names.index(true_target)\n",
    "            matrix[row][col] = 1\n",
    "        \n",
    "        target_names_with_count = []\n",
    "        for i in range(len(matrix)):\n",
    "            row_sum = sum(matrix[i])\n",
    "            target_names_with_count.append(\"{} ({})\".format(target_names[i], int(row_sum)))\n",
    "            for j in range(len(matrix[i])):\n",
    "                matrix[i][j] = matrix[i][j] / row_sum if row_sum != 0 else 0\n",
    "            \n",
    "        return matrix, target_names_with_count, target_names\n",
    "    \n",
    "    matrix, rows, columns = generate_prediction_matrix(prediction, target)\n",
    "    df_cm = pd.DataFrame(matrix, \n",
    "                         index = [i for i in rows],\n",
    "                         columns = [i for i in columns])\n",
    "\n",
    "    title = \"Epoch: {}, Precision: {}, Accuracy: {}, F1 {}\".format(epoch, precision, accuracy, f1)\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    confusion = sn.heatmap(df_cm, annot=True)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.savefig(\"confusion_matrix.png\")    \n",
    "    \n",
    "def f1(prediction, target, epoch): # not tensors but result values\n",
    "    \n",
    "    target = np.reshape(target, (-1, MAX_SENTENCE_LENGTH, NUM_TARGETS))\n",
    "    prediction = np.reshape(prediction, (-1, MAX_SENTENCE_LENGTH, NUM_TARGETS))\n",
    "    \n",
    "    \n",
    "    \n",
    "    tp=np.asarray([0]*(NUM_TARGETS+2))\n",
    "    fp=np.asarray([0]*(NUM_TARGETS+2))\n",
    "    fn=np.asarray([0]*(NUM_TARGETS+2))\n",
    "\n",
    "    target = np.argmax(target, 2)\n",
    "    prediction = np.argmax(prediction, 2)\n",
    "\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        for j in range(MAX_SENTENCE_LENGTH):\n",
    "            if target[i][j] == prediction[i][j]:\n",
    "                tp[target[i][j]] += 1\n",
    "            else:\n",
    "                fp[target[i][j]] += 1\n",
    "                fn[prediction[i][j]] += 1\n",
    "\n",
    "    NON_NAMED_ENTITY = 0\n",
    "    for i in range(NUM_TARGETS):\n",
    "        if i != NON_NAMED_ENTITY:\n",
    "            tp[5] += tp[i]\n",
    "            fp[5] += fp[i]\n",
    "            fn[5] += fn[i]\n",
    "        else:\n",
    "            tp[6] += tp[i]\n",
    "            fp[6] += fp[i]\n",
    "            fn[6] += fn[i]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    for i in range(NUM_TARGETS+2):\n",
    "        precision.append(tp[i]*1.0/(tp[i]+fp[i]))\n",
    "        recall.append(tp[i]*1.0/(tp[i]+ fn[i]))\n",
    "        fscore.append(2.0*precision[i]*recall[i]/(precision[i]+recall[i]))\n",
    "    \n",
    "    precision = sum(precision) / len(precision)\n",
    "    recall = sum(recall) / len(recall)\n",
    "    fscore = sum(fscore) / len(fscore)\n",
    "    \n",
    "    generate_confusion_matrix(prediction, target, precision, recall, fscore, epoch)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches pr. Epoch: 66\n",
      "\n",
      "Defining variables\n",
      "Initializing variables\n",
      "starting training"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define Model Parameters\n",
    "\"\"\"\n",
    "\n",
    "NUM_HIDDEN = 100\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "\"\"\"\n",
    "Define Training Parameters\n",
    "\"\"\"\n",
    "BATCH_SIZE = 256\n",
    "BATCH_AMOUNT = (len(train_input) + BATCH_SIZE - 1) / BATCH_SIZE\n",
    "NUM_EPOCH = 200\n",
    "\n",
    "print(\"Batches pr. Epoch: {}\\n\".format(BATCH_AMOUNT))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    print(\"Defining variables\")\n",
    "    data = tf.placeholder(tf.float32,[None, MAX_SENTENCE_LENGTH, EMBEDDING_LENGTH])\n",
    "    target = tf.placeholder(tf.float32, [None, MAX_SENTENCE_LENGTH, NUM_TARGETS])\n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "    \n",
    "    model = Model(data, target, dropout, NUM_HIDDEN, NUM_LAYERS, LEARNING_RATE)\n",
    "    maximum = 0\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Initializing variables\")\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        print(\"starting training\")\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "            ptr=0\n",
    "            for batch_number in range(BATCH_AMOUNT):\n",
    "                feed_input = {\n",
    "                        data: train_input[ptr:ptr+BATCH_SIZE], \n",
    "                        target: train_target[ptr:ptr+BATCH_SIZE], \n",
    "                        dropout: DROPOUT\n",
    "                }\n",
    " \n",
    "                ptr += BATCH_SIZE\n",
    "                sess.run(model.optimize, feed_input)\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "                #print(\"Model saved in file: %s\" % save_path)\n",
    "            #pred = sess.run(model.prediction, {data: dev_input, target: dev_target, dropout: 1})\n",
    "            \n",
    "            pred, length = sess.run(model.getpredf1, {data: dev_input, target: dev_target, dropout: 1})\n",
    "            #print \"Epoch: {} out of {}\".format(epoch, NUM_EPOCH), \"TestA score,\"\n",
    "            \n",
    "            m = f1(pred, dev_target, epoch)\n",
    "            if m > maximum:\n",
    "                maximum = m\n",
    "                save_path = saver.save(sess, \"model/model_max.ckpt\")\n",
    "                #print(\"Max Model saved in file: %s\" % save_path)\n",
    "                \n",
    "                #pred = sess.run(model.prediction, {data: validation_input, target: validation_target, dropout: 1})\n",
    "                \n",
    "                pred, length = sess.run(model.getpredf1, {data: validation_input, target: validation_target, dropout: 1})\n",
    "                #print \"TestB score,\"\n",
    "                f1(pred, validation_target, epoch)\n",
    "                #print\"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
