{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "import random\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "from input import get_train_data,get_test_data,get_final_data\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "WORD_DIM = 313\n",
    "MAX_SEQ_LEN = 30\n",
    "NUM_CLASSES = 9\n",
    "BATCH_SIZE = 256\n",
    "NUM_HIDDEN = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_EPOCH = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data into memory\n",
      "test_a data loaded\n",
      "test_b data loaded\n",
      "train data loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load data into memory \n",
    "\"\"\"\n",
    "print \"loading data into memory\"\n",
    "pool = Pool(processes=3)\n",
    "train_result = pool.apply_async(get_train_data)\n",
    "test_a_result = pool.apply_async(get_test_data)\n",
    "test_b_result = pool.apply_async(get_final_data)\n",
    "\n",
    "test_inp, test_out = test_a_result.get()\n",
    "print \"test_a data loaded\"\n",
    "\n",
    "final_inp, final_out = test_b_result.get()\n",
    "print \"test_b data loaded\"\n",
    "\n",
    "train_inp, train_out = train_result.get()\n",
    "print \"train data loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyse the data properly\n",
    "\"\"\"\n",
    "print([len(f) for f in train_out[:10]])\n",
    "\n",
    "no_of_batches = (len(train_inp) + BATCH_SIZE - 1) / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the tensorflow model used to train the NER reacogniser\n",
    "\"\"\"\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, data, target, dropout, num_hidden, num_layers):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.dropout = dropout\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        rnn_cell = tf.nn.rnn_cell\n",
    "        # Try: LSTMBlock cell or GruBlock cell\n",
    "        fw_cell = rnn_cell.LSTMCell(self._num_hidden, state_is_tuple=True)\n",
    "        bw_cell = rnn_cell.LSTMCell(self._num_hidden, state_is_tuple=True)\n",
    "\n",
    "        if self._num_layers > 1:\n",
    "            fw_cell = rnn_cell.MultiRNNCell([fw_cell] * self._num_layers, state_is_tuple=True)\n",
    "            fw_cell = rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=self.dropout)\n",
    "            bw_cell = rnn_cell.MultiRNNCell([bw_cell] * self._num_layers, state_is_tuple=True)\n",
    "            bw_cell = rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=self.dropout)\n",
    "        else:\n",
    "            fw_cell = rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=self.dropout)\n",
    "            bw_cell = rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=self.dropout)\n",
    "\n",
    "        # Try: Dynamic Bidirectional RNN\n",
    "        output, _, _ = tf.nn.bidirectional_rnn(fw_cell, \n",
    "                                               bw_cell, \n",
    "                                               tf.unpack(tf.transpose(self.data, perm=[1, 0, 2])), \n",
    "                                               dtype=tf.float32, \n",
    "                                               sequence_length=self.length)\n",
    "\n",
    "\n",
    "        max_length = int(self.target.get_shape()[1])\n",
    "        num_classes = int(self.target.get_shape()[2])\n",
    "        weight, bias = self._weight_and_bias(2*self._num_hidden, num_classes)\n",
    "        output = tf.reshape(tf.transpose(tf.pack(output), perm=[1, 0, 2]), [-1, 2 * self._num_hidden])\n",
    "        prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)\n",
    "        prediction = tf.reshape(prediction, [-1, max_length, num_classes])\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = self.target * tf.log(self.prediction)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2) # Summarize the values of 2 axis\n",
    "        \n",
    "        # Check if the maximum value on the secondary axis is positive or negative\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.target), reduction_indices=2)) \n",
    "        cross_entropy *= mask # Ensure the cross_entropy is positive (by multiplying either with -1 or 1)\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1) # Summarize the values on the primary axis\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32) # Convert all dimensions of the vector to 32float.\n",
    "        return tf.reduce_mean(cross_entropy) # Reduce the vector to the mean value on all dimensions\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        optimizer = tf.train.AdamOptimizer(0.003)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 2), tf.argmax(self.prediction, 2))\n",
    "\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.target), reduction_indices=2))\n",
    "        mistakes *= mask\n",
    "        # Average over actual sequence lengths.\n",
    "        mistakes = tf.reduce_sum(mistakes, reduction_indices=1)\n",
    "        mistakes /= tf.cast(self.length, tf.float32)\n",
    "        return tf.reduce_mean(mistakes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size,out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @lazy_property\n",
    "    def getpredf1(self):\n",
    "        return self.prediction, self.length\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\"\"\"\n",
    "Define the methods needed to train the model\n",
    "\"\"\"\n",
    "def f1(prediction,target,length):\n",
    "    tp=np.array([0]*(NUM_CLASSES+1))\n",
    "    fp=np.array([0]*(NUM_CLASSES+1))\n",
    "    fn=np.array([0]*(NUM_CLASSES+1))\n",
    "\n",
    "    target = np.argmax(target, 2)\n",
    "    prediction = np.argmax(prediction, 2)\n",
    "\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        for j in range(length[i]):\n",
    "            if target[i][j] == prediction[i][j]:\n",
    "                tp[target[i][j]] += 1\n",
    "            else:\n",
    "                fp[target[i][j]] += 1\n",
    "                fn[prediction[i][j]] += 1\n",
    "\n",
    "    NON_NAMED_ENTITY = 11\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if i != NON_NAMED_ENTITY:\n",
    "            tp[NUM_CLASSES] += tp[i]\n",
    "            fp[NUM_CLASSES] += fp[i]\n",
    "            fn[NUM_CLASSES] += fn[i]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    for i in range(NUM_CLASSES+1):\n",
    "        precision.append(tp[i]*1.0/(tp[i]+fp[i]))\n",
    "        recall.append(tp[i]*1.0/(tp[i]+ fn[i]))\n",
    "        fscore.append(2.0*precision[i]*recall[i]/(precision[i]+recall[i]))\n",
    "\n",
    "    print \"precision = {}\".format([\"{:10.4f}%\".format(f) for f in precision])\n",
    "    print \"recall = {}\".format([\"{:10.4f}%\".format(f) for f in recall])\n",
    "    print \"f1score = {}\".format([\"{:10.4f}%\".format(f) for f in fscore])\n",
    "                            \n",
    "    return fscore[NUM_CLASSES]\n",
    "\n",
    "\n",
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        print(\"defining variables\")\n",
    "        data = tf.placeholder(tf.float32,[None, MAX_SEQ_LEN, WORD_DIM])\n",
    "        target = tf.placeholder(tf.float32, [None, MAX_SEQ_LEN, NUM_CLASSES])\n",
    "        dropout = tf.placeholder(tf.float32)\n",
    "        model = Model(data,target,dropout,NUM_HIDDEN,NUM_LAYERS)\n",
    "        maximum = 0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            print(\"initializing variables\")\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            print(\"starting training\")\n",
    "            for epoch in range(200):\n",
    "                ptr=0\n",
    "                for batch_number in range(no_of_batches):\n",
    "                    batch_inp, batch_out = train_inp[ptr:ptr+BATCH_SIZE], train_out[ptr:ptr+BATCH_SIZE]\n",
    "                    ptr += BATCH_SIZE\n",
    "                    sess.run(model.optimize,{data: np.array(batch_inp),\n",
    "                                             target: np.array(batch_out),\n",
    "                                             dropout: 0.5})\n",
    "                if epoch % 10 == 0:\n",
    "                    save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "                pred = sess.run(model.prediction, {data: test_inp, target: test_out, dropout: 1})\n",
    "                pred,length = sess.run(model.getpredf1, {data: test_inp, target: test_out, dropout: 1})\n",
    "                print \"Epoch:\" + str(epoch), \"TestA score,\"\n",
    "                m = f1(pred,test_out,length)\n",
    "                if m > maximum:\n",
    "                    maximum = m\n",
    "                    save_path = saver.save(sess, \"model/model_max.ckpt\")\n",
    "                    print(\"Max Model saved in file: %s\" % save_path)\n",
    "                    pred = sess.run(model.prediction, {data: final_inp, target: final_out, dropout: 1})\n",
    "                    pred,length = sess.run(model.getpredf1, {data: final_inp, target: final_out, dropout: 1})\n",
    "                    print \"TestB score,\"\n",
    "                    f1(pred,final_out,length)\n",
    "                    print\"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining variables\n",
      "initializing variables\n",
      "starting training\n",
      "Model saved in file: model/model.ckpt\n",
      "Epoch:0 TestA score,\n",
      "precision =  ['    0.0000%', '    0.0000%', '    1.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.9015%']\n",
      "recall =  ['       nan%', '       nan%', '    0.9015%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '    0.9015%']\n",
      "f1score =  ['       nan%', '       nan%', '    0.9482%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '    0.9015%']\n",
      "Max Model saved in file: model/model_max.ckpt\n",
      "TestB score,\n",
      "precision =  ['    0.0000%', '    0.0000%', '    1.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.0000%', '    0.9164%']\n",
      "recall =  ['       nan%', '       nan%', '    0.9164%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '    0.9164%']\n",
      "f1score =  ['       nan%', '       nan%', '    0.9564%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '       nan%', '    0.9164%']\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-27f1cf57d6c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mTrain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-c871cb6eda19>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                     sess.run(model.optimize,{data: np.array(batch_inp),\n\u001b[1;32m     65\u001b[0m                                              \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                                              dropout: 0.5})\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Vilstrup/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the model\n",
    "\"\"\"\n",
    "train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
