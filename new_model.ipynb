{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26100 unique words (264715 in total)\n",
      "Found 91 unique characters\n",
      "Found 17 unique named entity tags\n",
      "8323 / 1915 / 1517 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Import standard libraires\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "import re\n",
    "import scipy.io\n",
    "import codecs\n",
    "import cPickle\n",
    "\n",
    "# Import Tensorflow, and corresponding functions\n",
    "import tensorflow as tf\n",
    "from utils import set_values, get_name\n",
    "\n",
    "\n",
    "# Import custom helper functions\n",
    "from utils import create_input\n",
    "import loader\n",
    "from utils import models_path, evaluate, eval_script, eval_temp\n",
    "from loader import word_mapping, char_mapping, tag_mapping\n",
    "from loader import update_tag_scheme, prepare_dataset\n",
    "from loader import augment_with_pretrained\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data Variables\n",
    "\"\"\"\n",
    "train=\"data/esp.train\" \n",
    "dev=\"data/esp.testa\" \n",
    "test=\"data/esp.testb\"\n",
    "tag_scheme=\"iobes\"         # Tagging scheme (IOB or IOBES)\n",
    "pre_emb=\"\"                 # Location of pretrained embeddings\n",
    "all_emb=False              # Load all embeddings\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model Variables\n",
    "\"\"\"\n",
    "lower=False                # Lowercase words (this will not affect character inputs)\n",
    "zeros=False                # Replace digits with 0\n",
    "char_dim=25                # Char embedding dimension\n",
    "char_lstm_dim=25           # Char LSTM hidden layer size\n",
    "char_bidirect=True         # Use a bidirectional LSTM for chars\n",
    "word_dim=100               # Token embedding dimension\n",
    "word_lstm_dim=100          # Token LSTM hidden layer size\n",
    "word_bidirect=True         # Use a bidirectional LSTM for words\n",
    "cap_dim=0                  # Capitalization feature dimension (0 to disable)\n",
    "crf=True                   # Use CRF (False to disable)\n",
    "dropout=0.5                # Droupout on the input (0 = no dropout)\n",
    "lr_method=\"sgd-lr_.005\"    # Learning method (SGD, Adadelta, Adam..)\n",
    "reload_model=0             # Reload the last saved model\n",
    "\n",
    "\"\"\"\n",
    "Training variables\n",
    "\"\"\"\n",
    "singletons = set([word_to_id[k] for k, v\n",
    "                  in dico_words_train.items() if v == 1])\n",
    "n_epochs = 100  # number of epochs over the training set\n",
    "freq_eval = 1000  # evaluate on dev every freq_eval steps\n",
    "\n",
    "\n",
    "# Parse parameters\n",
    "parameters = OrderedDict()\n",
    "parameters['tag_scheme']    = tag_scheme\n",
    "parameters['lower']         = lower\n",
    "parameters['zeros']         = zeros\n",
    "parameters['char_dim']      = char_dim\n",
    "parameters['char_lstm_dim'] = char_lstm_dim\n",
    "parameters['char_bidirect'] = char_bidirect\n",
    "parameters['word_dim']      = word_dim\n",
    "parameters['word_lstm_dim'] = word_lstm_dim\n",
    "parameters['word_bidirect'] = word_bidirect\n",
    "parameters['pre_emb']       = pre_emb\n",
    "parameters['all_emb']       = all_emb\n",
    "parameters['cap_dim']       = cap_dim\n",
    "parameters['crf']           = crf\n",
    "parameters['dropout']       = dropout\n",
    "parameters['lr_method']     = lr_method\n",
    "\n",
    "# Check parameters validity\n",
    "assert os.path.isfile(train)\n",
    "assert os.path.isfile(dev)\n",
    "assert os.path.isfile(test)\n",
    "assert parameters['char_dim'] > 0 or parameters['word_dim'] > 0\n",
    "assert 0. <= parameters['dropout'] < 1.0\n",
    "assert parameters['tag_scheme'] in ['iob', 'iobes']\n",
    "assert not parameters['all_emb'] or parameters['pre_emb']\n",
    "assert not parameters['pre_emb'] or parameters['word_dim'] > 0\n",
    "assert not parameters['pre_emb'] or os.path.isfile(parameters['pre_emb'])\n",
    "\n",
    "# Check evaluation script / folders\n",
    "if not os.path.isfile(eval_script):\n",
    "    raise Exception('CoNLL evaluation script not found at \"%s\"' % eval_script)\n",
    "if not os.path.exists(eval_temp):\n",
    "    os.makedirs(eval_temp)\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)\n",
    "\n",
    "\n",
    "# Data parameters\n",
    "lower = parameters['lower']\n",
    "zeros = parameters['zeros']\n",
    "tag_scheme = parameters['tag_scheme']\n",
    "\n",
    "# Load sentences\n",
    "train_sentences = loader.load_sentences(train, lower, zeros)\n",
    "dev_sentences = loader.load_sentences(dev, lower, zeros)\n",
    "test_sentences = loader.load_sentences(test, lower, zeros)\n",
    "\n",
    "# Use selected tagging scheme (IOB / IOBES)\n",
    "update_tag_scheme(train_sentences, tag_scheme)\n",
    "update_tag_scheme(dev_sentences, tag_scheme)\n",
    "update_tag_scheme(test_sentences, tag_scheme)\n",
    "\n",
    "# Create a dictionary / mapping of words\n",
    "# If we use pretrained embeddings, we add them to the dictionary.\n",
    "if parameters['pre_emb']:\n",
    "    dico_words_train = word_mapping(train_sentences, lower)[0]\n",
    "    dico_words, word_to_id, id_to_word = augment_with_pretrained(\n",
    "        dico_words_train.copy(),\n",
    "        parameters['pre_emb'],\n",
    "        list(itertools.chain.from_iterable(\n",
    "            [[w[0] for w in s] for s in dev_sentences + test_sentences])\n",
    "        ) if not parameters['all_emb'] else None\n",
    "    )\n",
    "else:\n",
    "    dico_words, word_to_id, id_to_word = word_mapping(train_sentences, lower)\n",
    "    dico_words_train = dico_words\n",
    "\n",
    "# Create a dictionary and a mapping for words / POS tags / tags\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "\n",
    "# Index data\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, lower\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, char_to_id, tag_to_id, lower\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, lower\n",
    ")\n",
    "\n",
    "print \"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "    len(train_data), len(dev_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caps': [2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      " 'chars': [[36, 0, 7, 15, 2, 11, 5, 3, 0],\n",
      "           [47],\n",
      "           [27, 11, 4, 9, 5, 1, 7, 6, 1],\n",
      "           [48],\n",
      "           [14],\n",
      "           [38, 56],\n",
      "           [13, 1, 20],\n",
      "           [47],\n",
      "           [22, 42, 22],\n",
      "           [48],\n",
      "           [18]],\n",
      " 'str_words': [u'Melbourne',\n",
      "               u'(',\n",
      "               u'Australia',\n",
      "               u')',\n",
      "               u',',\n",
      "               u'25',\n",
      "               u'may',\n",
      "               u'(',\n",
      "               u'EFE',\n",
      "               u')',\n",
      "               u'.'],\n",
      " 'tags': [2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      " 'words': [5666, 17, 2104, 16, 2, 193, 33, 17, 28, 16, 5]}\n",
      "[5666, 17, 2104, 16, 2, 193, 33, 17, 28, 16, 5]\n",
      "[[36, 0, 7, 15, 2, 11, 5, 3, 0], [47, 0, 0, 0, 0, 0, 0, 0, 0], [27, 11, 4, 9, 5, 1, 7, 6, 1], [48, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0], [38, 56, 0, 0, 0, 0, 0, 0, 0], [13, 1, 20, 0, 0, 0, 0, 0, 0], [47, 0, 0, 0, 0, 0, 0, 0, 0], [22, 42, 22, 0, 0, 0, 0, 0, 0], [48, 0, 0, 0, 0, 0, 0, 0, 0], [18, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[0, 3, 5, 11, 2, 15, 7, 0, 36], [47, 0, 0, 0, 0, 0, 0, 0, 0], [1, 6, 7, 1, 5, 9, 4, 11, 27], [48, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0], [56, 38, 0, 0, 0, 0, 0, 0, 0], [20, 1, 13, 0, 0, 0, 0, 0, 0], [47, 0, 0, 0, 0, 0, 0, 0, 0], [22, 42, 22, 0, 0, 0, 0, 0, 0], [48, 0, 0, 0, 0, 0, 0, 0, 0], [18, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[8, 0, 8, 0, 0, 1, 2, 0, 2, 0, 0]\n",
      "[2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_data[0])\n",
    "for array in create_input(train_data[0], parameters, True, singletons):\n",
    "    print array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    Network architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters=None, models_path=None, model_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the model. We either provide the parameters and a path where\n",
    "        we store the models, or the location of a trained model.\n",
    "        \"\"\"\n",
    "        # If there is no existing model create it\n",
    "        if model_path is None: \n",
    "            assert parameters and models_path\n",
    "            # Create a name based on the parameters\n",
    "            self.parameters = parameters\n",
    "            self.name = get_name(parameters)\n",
    "            # Model location\n",
    "            model_path = os.path.join(models_path, self.name)\n",
    "            self.model_path = model_path\n",
    "            self.parameters_path = os.path.join(model_path, 'parameters.pkl')\n",
    "            self.mappings_path = os.path.join(model_path, 'mappings.pkl')\n",
    "            # Create directory for the model if it does not exist\n",
    "            if not os.path.exists(self.model_path):\n",
    "                os.makedirs(self.model_path)\n",
    "            # Save the parameters to disk\n",
    "            with open(self.parameters_path, 'wb') as f:\n",
    "                cPickle.dump(parameters, f)\n",
    "        # Else load an existing model into memory\n",
    "        else:\n",
    "            assert parameters is None and models_path is None\n",
    "            # Model location\n",
    "            self.model_path = model_path\n",
    "            self.parameters_path = os.path.join(model_path, 'parameters.pkl')\n",
    "            self.mappings_path = os.path.join(model_path, 'mappings.pkl')\n",
    "            # Load the parameters and the mappings from disk\n",
    "            with open(self.parameters_path, 'rb') as f:\n",
    "                self.parameters = cPickle.load(f)\n",
    "            self.reload_mappings()\n",
    "        self.components = {}\n",
    "\n",
    "    def save_mappings(self, id_to_word, id_to_char, id_to_tag):\n",
    "        \"\"\"\n",
    "        We need to save the mappings if we want to use the model later.\n",
    "        \"\"\"\n",
    "        self.id_to_word = id_to_word\n",
    "        self.id_to_char = id_to_char\n",
    "        self.id_to_tag = id_to_tag\n",
    "        with open(self.mappings_path, 'wb') as f:\n",
    "            mappings = {\n",
    "                'id_to_word': self.id_to_word,\n",
    "                'id_to_char': self.id_to_char,\n",
    "                'id_to_tag': self.id_to_tag,\n",
    "            }\n",
    "            cPickle.dump(mappings, f)\n",
    "\n",
    "    def reload_mappings(self):\n",
    "        \"\"\"\n",
    "        Load mappings from disk.\n",
    "        \"\"\"\n",
    "        with open(self.mappings_path, 'rb') as f:\n",
    "            mappings = cPickle.load(f)\n",
    "        self.id_to_word = mappings['id_to_word']\n",
    "        self.id_to_char = mappings['id_to_char']\n",
    "        self.id_to_tag = mappings['id_to_tag']\n",
    "\n",
    "    def add_component(self, param):\n",
    "        \"\"\"\n",
    "        Add a new parameter to the network.\n",
    "        \"\"\"\n",
    "        if param.name in self.components:\n",
    "            raise Exception('The network already has a parameter \"%s\"!'\n",
    "                            % param.name)\n",
    "        self.components[param.name] = param\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Write components values to disk.\n",
    "        \"\"\"\n",
    "        for name, param in self.components.items():\n",
    "            param_path = os.path.join(self.model_path, \"%s.mat\" % name)\n",
    "            if hasattr(param, 'params'):\n",
    "                param_values = {p.name: p.get_value() for p in param.params}\n",
    "            else:\n",
    "                param_values = {name: param.get_value()}\n",
    "            scipy.io.savemat(param_path, param_values)\n",
    "\n",
    "    def reload(self):\n",
    "        \"\"\"\n",
    "        Load components values from disk.\n",
    "        \"\"\"\n",
    "        for name, param in self.components.items():\n",
    "            param_path = os.path.join(self.model_path, \"%s.mat\" % name)\n",
    "            param_values = scipy.io.loadmat(param_path)\n",
    "            if hasattr(param, 'params'):\n",
    "                for p in param.params:\n",
    "                    set_values(p.name, p, param_values[p.name])\n",
    "            else:\n",
    "                set_values(name, param, param_values[name])\n",
    "\n",
    "    def build(self,\n",
    "              dropout,\n",
    "              char_dim,\n",
    "              char_lstm_dim,\n",
    "              char_bidirect,\n",
    "              word_dim,\n",
    "              word_lstm_dim,\n",
    "              word_bidirect,\n",
    "              lr_method,\n",
    "              pre_emb,\n",
    "              crf,\n",
    "              cap_dim,\n",
    "              training=True,\n",
    "              **kwargs):\n",
    "        \"\"\"\n",
    "        Build the network.\n",
    "        \"\"\"\n",
    "        # Training parameters\n",
    "        n_words = len(self.id_to_word)\n",
    "        n_chars = len(self.id_to_char)\n",
    "        n_tags = len(self.id_to_tag)\n",
    "\n",
    "        # Number of capitalization features\n",
    "        if cap_dim: \n",
    "            n_cap = 4\n",
    "\n",
    "        # Network variables\n",
    "        word_ids = tf.placeholder(tf.int32, name='is_train')\n",
    "        char_for_ids = tf.placeholder(tf.int32, name='char_for_ids')\n",
    "        char_rev_ids = tf.placeholder(tf.int32, name='char_rev_ids')\n",
    "        char_pos_ids = tf.placeholder(tf.int32, name='char_pos_ids')\n",
    "        tag_ids = tf.placeholder(tf.int32, name='tag_ids')\n",
    "        \n",
    "        # Setting up placeholder, this is where your data enters the graph!\n",
    "        x_image_pl = tf.placeholder(tf.float32, [None, height, width, channels], name=\"x_image_pl\")\n",
    "        x_margin_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_margin_pl\")\n",
    "        x_shape_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_shape_pl\")\n",
    "        x_texture_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_texture_pl\")\n",
    "        is_training_pl = tf.placeholder(tf.bool, name=\"is_training_pl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start training the model\n",
    "\"\"\"\n",
    "\n",
    "best_dev = -np.inf\n",
    "best_test = -np.inf\n",
    "count = 0\n",
    "for epoch in xrange(n_epochs):\n",
    "    epoch_costs = []\n",
    "    print \"Starting epoch %i...\" % epoch\n",
    "    for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "        count += 1\n",
    "        input = create_input(train_data[index], parameters, True, singletons)\n",
    "        #new_cost = f_train(*input)\n",
    "        new_cost = 0\n",
    "        epoch_costs.append(new_cost)\n",
    "        if i % 50 == 0 and i > 0 == 0:\n",
    "            print \"%i, cost average: %f\" % (i, np.mean(epoch_costs[-50:]))\n",
    "        if count % freq_eval == 0:\n",
    "            dev_score = evaluate(parameters, f_eval, dev_sentences,\n",
    "                                 dev_data, id_to_tag, dico_tags)\n",
    "            test_score = evaluate(parameters, f_eval, test_sentences,\n",
    "                                  test_data, id_to_tag, dico_tags)\n",
    "            print \"Score on dev: %.5f\" % dev_score\n",
    "            print \"Score on test: %.5f\" % test_score\n",
    "            if dev_score > best_dev:\n",
    "                best_dev = dev_score\n",
    "                print \"New best score on dev.\"\n",
    "                print \"Saving model to disk...\"\n",
    "                model.save()\n",
    "            if test_score > best_test:\n",
    "                best_test = test_score\n",
    "                print \"New best score on test.\"\n",
    "    print \"Epoch %i done. Average cost: %f\" % (epoch, np.mean(epoch_costs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
